# Developing Bombastic

## Running locally

Requires `podman-compose`.

```shell
podman-compose -f compose.yaml up
```

This will start MinIO and Kafka in containers and initialize them accordingly so that you don't need to configure anything. Default arguments of Bombastic components will work with this setup.

## API

To run the API, you can use cargo:

```shell
RUST_LOG=info cargo run -p bombastic-api -- run --index api-index.sqlite --devmode
```

For searching using the index, more setup is required.

## Indexer

The indexer consumes events from Kafka and indexes SBOM entries:

```shell
RUST_LOG=info cargo run -p bombastic-indexer -- run --index indexer-index.sqlite --events kafka --devmode
```

At this point, you can POST and GET SBOMs with the API using the id. To ingest an SBOM:

```shell
curl -X POST --json @testdata/my-sbom.json http://localhost:8080/api/v1/sbom
```

Example for importing an SBOM generated by `syft`:

```shell
REGISTRY=registry.k8s.io/coredns IMAGE=coredns TAG=v1.9.3
podman pull $REGISTRY/$IMAGE:$TAG
PURL=pkg:oci/$NAME@sha256:$(podman images $REGISTRY/$IMAGE:$TAG --digests '--format={{.Id}}')
syft -q -o cyclonedx-json $REGISTRY/$IMAGE:$TAG | http --json POST http://localhost:8080/api/v1/sbom purl==$PURL
```

Or when pulling by digest:

```shell
REGISTRY=docker.io/bitnami
IMAGE=postgresql
DIGEST=e6d322cf36ff6b5e2bb13d71c816dc60f1565ff093cc220064dba08c4b057275

PURL=pkg:oci/$IMAGE@sha256:$DIGEST
syft -q -o spdx-json --name $IMAGE $REGISTRY/$IMAGE@sha256:$DIGEST | http --json POST http://localhost:8080/api/v1/sbom purl==$PURL sha256==$DIGEST
```

To query the data, either using direct lookup or querying via the index:

```shell
curl -X GET http://localhost:8080/api/v1/sbom?purl=pkg%3Amaven%2Fio.seedwing%2Fseedwing-java-example%401.0.0-SNAPSHOT%3Ftype%3Djar
curl -X GET http://localhost:8080/api/v1/sbom?purl=pkg%3Amaven%2Fio.seedwing%2Fseedwing-java-example%401.0.0-SNAPSHOT%3Ftype%3Djar
```

The indexer will automatically sync the index to the S3 bucket, while the API will periodically retrieve the index from S3. Therefore, there may be a delay between storing the entry and it being indexed.

## Exporter

TODO
